ЭФФЕКТИВНОСТЬ АЛГОРИТМА LU-РАЗЛОЖЕНИЯ  С ДВУХМЕРНЫМ ЦИКЛИЧЕСКИМ РАСПРЕДЕЛЕНИЕМ МАТРИЦЫ ДЛЯ ПАРАЛЛЕЛЬНОГО РЕШЕНИЯ УПРУГОПЛАСТИЧЕСКОЙ ЗАДАЧИ 

При решении упругопластических задач с большими пластическими деформациями методом конечных элементов требуется многократно решать систему линейных алгебраических уравнений с ленточной матрицей жесткости. Для распараллеливания решения данной системы уравненийприменена адаптация параллельного алгоритма решения систем линейных уравнений с заполненной матрицей с двухмерным циклическим распределением матрицы по процессорам для случая ленточной матрицы. В отличие от традиционного алгоритма для заполненных матриц в рассмотренном алгоритме предусмотрено хранение только тех блоков, в которых могут находиться ненулевые элементы. Это обеспечивает значительную экономию памяти посравнению с алгоритмом для заполненной матрицы с одновременным сохранением возможности двухмерного циклического распределения блоков матрицы по процессорам для сокращения объема и количества операций передачиданных между процессорами. Производительность алгоритма протестирована на решениях осесимметричной упругопластической задачи сжатия цилиндра и трехмерной упругопластической задачи сжатия параллелепипеда. Выполнен анализ эффективности метода для регулярных сеток разной размерности. Решение производилось на кластерной системе UMT Института математики и механики УрО РАН (г. Екатеринбург) с использованием MPI для передачиданных между процессорами.

 Упругопластическая задача с большими пластическими деформациями физически и геометрически существенно нелинейная 1. Для ее решения методом конечных элементов требуется значительное количество времени и возникает задача решения системы линейных алгебраических уравнений СЛАУ Axb с матрицей жесткости A и вектором b правой части относительно искомого вектора x обобщенной скорости в узлах конечно-элементной сетки. Существенно сократить время вычислений можно с помощью техники с параллельной архитектурой, к которой относятся, в частности, кластерные системы. Построение локальных матриц жесткости в методе конечных элементов может выполняться полностью параллельно, как и пересчет напряженно-деформированного состояния. Однако алгоритмизация распределенного параллельного решения СЛАУ является сложной задачей. Матрица A имеет ленточный вид с относительно небольшой шириной ленты. Ширина зависит как от типа аппроксимирующей расчетную область сетки, так и от способа нумерации ее узлов и является единственной основной характеристикой матрицы СЛАУ. В данной работе для рассматриваемого алгоритма решения системы уравнений с целью упрощения анализа результатов используются только регулярные сетки с одинаковым числом ячеек d вдоль каждой из сторон. В работе 2 была рассмотрена производительность трехдиагонального алгоритма LU-разложения при решении двухмерной упругопластической задачи. Использование его в трехмерной задаче приводит к появлению большого заполнения при вычислении LU-разложения, и алгоритм показывает плохую производительность. В работе 3 описан алгоритм решения СЛАУ с полностью заполненной матрицей с использованием LU-разложения с двухмерным циклическим распределением матрицы по процессорам, а в 4 дано описание модификации данного алгоритма для ленточной матрицы системы. Он базируется на алгоритме блочного LU-разложения 5 с использованием распределения матрицы по процессорам согласно работе 6. Модификация алгоритма заключается в том, что происходит хранение в памяти лишь тех блоков, в которых могут находиться ненулевые элементы матрицы. Целью работы является исследование эффективности распределенного параллельного алгоритма решения СЛАУ с двухмерным циклическим распределением матрицы в упругопластической задаче. Алгоритм параллельного решения СЛАУ Использование блочной формы хранения позволяет получить меньший объем передачи данных для каждого процессора и большую эффективность использования процессорного кэша 7. При инициализации алгоритма строится структура данных, хранящая индекс первого столбца блочной строки в массиве блоков, номер столбца первого ненулевого блока и количество ненулевых блоков в строке. Каждый процессор в процессорной сетке должен иметь возможность выполнять массовые операции внутри своей строки или столбца. Это реализовано пользовательскими коммуникаторами MPI 8, в которых находятся только процессоры из конкретного столбца или строки процессорной сетки. Алгоритм параллельного решения СЛАУ можно разделить на несколько стадий разбиение матрицы на блоки, нумерация блоков матрицы, перестановка блоков матрицы, формирование локальных частей матрицы, вычисление LU-разложения, решение системы с разложенной матрицей. Разбиение матрицы на блоки. Матрица разбивается на квадратные блоки размерности n n. При вычислении на платформах x86 и x8664 компании Intel для достижения большей производительности инструкция по оптимизации 9 рекомендует использовать базовые адреса для массивов, кратные 16, и для типа данных double иметь четное количество элементов в строке при использовании схемы хранения матриц с ведущей строкой. Поэтому использована степень 2 в качестве значений n. Нумерация блоков матрицы. Каждому блоку матрицы ставится в соответствие номер n. Данная нумерация в общем случае может быть произвольной. Кроме этого, каждому блоку матрицы дополнительно по простой схеме циклического распределения 6 присваивается номер процессора n, на котором будут производиться вычисления над этим блоком. Пример разбиения матрицы на блоки и нумерации блоков номерами n показан на рисунке 1. Перестановка блоков матрицы. На базе номеров n и n, полученных на этапе нумерации блоков матрицы, формируется список пар . Данные пары на каждом процессоре сортируются по номеру процессора n для объединения блоков матрицы в группы с одинаковыми номерами процессоров n. Далее над массивом блоков выполняется операция перестановки, задаваемая номерами n из отсортированного списка пар . Целью перестановки является создание групп блоков для их пересылки на другие процессоры, номера которых определяются номерами n. Формирование локальных частей матрицы. На данном этапе производится пересылка групп блоков, сформированных на предыдущем этапе. Сначала вычисляется количество блоков в каждой из групп, и эти данные передаются процессору с номером n. Затем передаются сами блоки. Для передачи данной информации используется операция MPIAlltoallv. Вычисление LU-разложения. После получения процессорами частей матрицы СЛАУ применяется алгоритм блочного LU-разложения. Этот алгоритм является обобщением алгоритма LUразложения 5. Пусть матрица СЛАУ A размерности n, кратной n, разбита на блоки A размерности n n. Так как на процессорах хранятся только блоки матрицы A с ненулевыми элементами, i-й шаг алгоритма выполняется с блочной матрицей B наибольшего размера, содержащей только ненулевые блоки, элементом B которой является блок A. На рисунке 2 жирными линиями выделена матрица B, у которой блоком B является блок A. Сначала вычисляется LU-разложение блока B, которое рассылается процессорам, находящимся в одинаковых столбцах и строках процессорной сетки. Внутри столбца процессорной сетки блоки матрицы L, а внутри строки процессорной сетки блоки матрицы U. Полученные блоки матрицы L рассылаются по строкам процессорной сетки, а блоки матрицы U по столбцам процессорной сетки. После получения необходимых данных каждый процессор вычисляет дополнение Шура для оставшихся блоков матрицы B. Решение системы с разложенной матрицей. Решение системы состоит из шагов прямой и обратной подстановок. Эти шаги симметричны друг другу с учетом того, что в случае прямой подстановки используется поддиагональная часть L блока, стоящего на главной диагонали матрицы L, а в случае обратной наддиагональная часть U матрицы U. Результаты вычислительных экспериментов Вычислительные эксперименты проводились на кластере UMT Института математики и механики УрО РАН . Каждый из 208 вычислительных узлов был оборудован двумя 4-ядерными процессорами Intel Xeon E5450 и 16 ГБ оперативной памяти. Узлы соединены межL U ду собой Infiniband для передачи сообщений и Gigabit Ethernet для ввода-вывода. Для компиляции использовался компилятор Intel, для передачи сообщений OpenMPI. Решение основано на принципе виртуальной мощности в скоростной форме 1. Относительно контакта с плитами было принято условие прилипания металла к ним. Нагрузка в виде перемещения плиты прикладывалась малыми шагами h. Шаг h выбирался таким, чтобы отношение hh не превышало предел упругости по деформации, в данном случае 0,002, что обеспечивает устойчивость вычислительной процедуры. Величина относительного сжатия параллелепипеда и цилиндра принята равной 0,5. На каждом шаге нагрузки задача рассматривалась как квазистатическая, а вариационное равенство принципа виртуальной мощности в скоростной форме с помощью конечно-элементной аппроксимации сводилось к СЛАУ. При распараллеливании формирования глобальной матрицы жесткости конечные элементы распределялись слоями между процессорами. При этом операция объединения матрицы жесткости осуществлялась только с соседними процессорами. При таком распараллеливании количество процессоров не может превышать количество слоев конечных элементов. Вычислительный эксперимент для задачи сжатия параллелепипеда производился с параметрами, представленными в таблице 1. Зависимость ускорения вычисления LU-разложения и решения СЛАУ в упругопластической задаче сжатия параллелепипеда от размера процессорной сетки показана на рисунке 3. На ней время, затрачиваемое на передачу данных, начинает превышать время, затрачиваемое на вычисления, и наблюдается замедление при использовании 8 процессоров. При вариации неквадратных сеток, то есть замены, например, 12 на 21 и 24 на 42, не происходит существенное изменение ускорения. Отсюда можно заключить, что ориентация неквадратных процессорных сеток практически не влияет на производительность LU-разложения. Из рисунка 3б видно, что производительность алгоритма решения СЛАУ зависит от количества столбцов в процессорной сетке. Зубчатый характер рисунка обусловлен тем, что происходит чередование сеток с большим и меньшим количеством столбцов в процессорной сетке. Вычислительный эксперимент для двухмерной упругопластической задачи сжатия цилиндра выполнялся с параметрами, представленными в таблице 2. Зависимость ускорения при вычислении LUразложения и решении СЛАУ в этой задаче показана на рисунке 4. Из него видно, что при увеличении количества разбиений d увеличивается ускорение при вычислении как LU-разложения, так и решения СЛАУ. Полуширина матриц в двухмерной задаче невелика, и при увеличении процессорной сетки требуется уменьшать размеры блоков и локальных частей матрицы. В экстремальных точках графиков ускорений имеет место преобладание времени, затрачиваемого на передачу данных, над временем, затрачиваемым на вычисления. Падение всех линий графика для решения СЛАУ определяется тем, что на процессорной сетке 22 появляется передача данных внутри столбцов сетки. В трехмерной задаче полуширина ленты матрицы больше, чем в двухмерном случае, поэтому на рассмотренных процессорных сетках выигрыш по времени от использования большего количества процессоров превышает временные затраты на межпроцессорное взаимодействие. Так как количество элементов в матрице жесткости трехмерной задачи растет гораздо быстрее, чем в двухмерной, и ширина ленты матрицы жесткости получается больше, чем в случае двухмерной задачи, рассматриваемый алгоритм показывает лучшие результаты в трехмерном случае. Использование большого количества процессоров в трехмерной задаче было ограничено выбранным распределением конечных элементов по процессорам слоями. Если для двухмерной задачи это было оправдано тем, что количество элементов в слое сравнительно небольшое, то в трехмерной задаче в слое находится большое количество конечных элементов, а число самих слоев невелико. Так как количество начальных ненулевых элементов в строке матрицы жесткости не зависит от числа конечных элементов, в дальнейшем представляется целесообразным использовать разряженные схемы хранения матрицы и подобный подход к распараллеливанию этапа построения матрицы жесткости. Это позволит более гибко распределять конечные элементы между процессорами. На основании изложенного можно сделать следующие выводы. При решении СЛАУ в трехмерной упругопластической задаче алгоритм LUразложения с двухмерным циклическим распределением матрицы показал рост производительности с увеличением числа процессоров. В двухмерной задаче алгоритм показал рост производительности лишь на процессорных сетках меньше 44. Это было обусловлено небольшим размером полуширины ленты матрицы жесткости, из-за чего пересылка блоков между процессорами занимала больше времени, чем вычисления над блоками. 