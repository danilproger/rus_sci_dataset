ЭКСПЕРИМЕНТАЛЬНАЯ СРЕДА ОБЛАЧНЫХ ВЫЧИСЛЕНИЙ  В ИНСТИТУТЕ МАТЕМАТИКИ И МЕХАНИКИ УРО РАН 

Рассматривается задача предоставления масштабируемой вычислительной инфраструктуры для консолидации компьютерного оборудования, используемого в целях разработки, отладки и развертывания ПО, а также дидактической поддержки образовательных курсов. Сформулированы требования к ПО и выполнен обзор существующих решений: Eucalyptus, OpenNebula, OpenStack, Nimbus. В результате их сравнения принято решение об экспериментальном развертывании средств OpenNebula на основе гипервизора KVM и ОС GNU/Linux. Среда OpenNebula – это свободное ПО, предназначенное для управления облачной инфраструктурой класса «инфраструктура как услуга». Гипервизор виртуальных машин KVM разрабатывается компанией Red Hat и поддерживается средой OpenNebula в качестве решения для виртуализации по умолчанию. Описаны вычислительные ресурсы Института математики и механики (ИММ) УрО РАН и выделенные серверные узлы для запуска экспериментальной среды облачных вычислений. Приведена конфигурация типового узла среды облачных вычислений на основе ОС Scientific Linux 6.1 (x86_64) с использованием технологий LVM, KVM и libvirt. Описано представление облачной среды как на логическом, так и на физическом уровнях. Обозначены проблемы функционирования системы безопасности SELinux и обеспечения отказоустойчивости системы. Сформулирован дальнейший план работ по таким направлениям, как интеграция с доменной системой Active Directory, применение сетевого хранилища (NAS) для хранения образов виртуальных машин, миграция на клиентсерверную БД MySQL и необходимость апробации и предоставления масштабируемых Web-сервисов для конечных пользователей среды облачных вычислений ИММ УрО РАН. 

 Вычислительный центр Института математики и механики ИММ УрО РАН должен одновременно решать две взаимоисключающие задачи предоставление услуг научным сотрудникам институтов УрО РАН и проведение собственных исследований и разработок. Предоставление услуг требует высокой надежности всех компонентов центра, высокой доступности сервисов и служб. Проведение собственных исследований и разработок, наоборот, может приводить к непредвиденным последствиям, результатом которых могут стать длительные перерывы в работе. Вычислительные кластеры ИММ работают под управлением ОС GNULinux. Время от времени возникают нетипичные задачи, требующие переконфигурирования кластера или выделения вычислительных ресурсов на короткий срок. Например, для разработки и отладки некоторого приложения может потребоваться кластер Windows. Такой кластер не будет сильно нагружен, поэтому создание специального кластера или обеспечение двойной загрузки представляется нерациональным. В рамках учебных курсов совместной кафедры УрФУ и ИММ УрО РАН Высокопроизводительные компьютерные технологии требуются средства для проведения практических занятий со студентами по развертыванию кластерного ПО. Кроме того, в институте существует набор унаследованных устаревших серверов разного типа, не несущих заметной нагрузки, и было бы целесообразно сократить количество используемых ими физических установок. В отличие от проблем, возникающих при экспериментах с оборудованием, вышеперечисленные задачи могут быть решены за счет развертывания специальной компьютерной инфраструктуры, объединяющей некоторое количество физических серверов в единую систему, ресурсы которой разделяются на виртуальные машины, используемые многими пользователями одновременно, причем пользователи самостоятельно настраивают их под свои задачи. Пользователям предоставляется Web-интерфейс, позволяющий создавать вычислительные ресурсы при возникновении потребности в них и удалять их в случае ненужности. В настоящее время решения такого типа классифицируются как облачные среды IaaS . Для изучения возможностей данной технологии было принято решение о развертывании среды облачных вычислений. Выбор программных продуктов ПО должно быть свободно распространяемым и с открытым кодом, а также иметь возможность миграции виртуальных машин между серверами кластера виртуализации, балансировки нагрузки, создания виртуальных машин по готовым шаблонам, загрузки образов виртуальных машин, созданных пользователями вне IaaS-системы, и преобразования форматов различных систем виртуальных машин. В настоящее время разработчики предлагают ряд продуктов, реализующих концепцию IaaS поверх различных систем виртуальных машин. Сравнение средств для создания IaaS-систем дается в работах 1, 2. В качестве базовой среды для облачных вычислений в СКЦ ИММ УрО РАН был выбран продукт OpenNebula. Разработка OpenNebula стартовала в 2005 г.как исследовательский проект Distributed Systems Architecture Research Group в Мадридском университете Комплутенсе . В марте 2008 г.состоялся первый релиз продукта. OpenNebula используется рядом европейских научно-исследовательских организаций, среди которых отдельно следует упомянуть CERN. Программное обеспечение OpenNebula предназначено для управления облачной инфраструктурой класса IaaS. В качестве системы виртуализации возможно использование Xen, KVM, VMware и Hyper-V. Программные средства OpenNebula поддерживают API для доступа к публичным облачным окружениям, таким как Amazon EC2 Query, OGF OCCI, vCloud и др. Таким образом, OpenNebula может использоваться для создания частных облачных сред, обеспечивать работу только с внешними, публичными облачными сервисами таких провайдеров, как Amazon EC2, а также развертывать гибридные облачные системы, сочетающие сервисы публичных и частных инфраструктур. Сравнив средства виртуализации в работах 3 и 4, авторы остановили свой выбор на средстве построения среды облачных вычислений OpenNebula и системе виртуализации KVM. В целом KVM является более предпочтительной по результатам тестов производительности подсистемы памяти и ЦП, чем Xen. Кроме того, в Xen могут возникать некоторые сложности с многопоточными программами, что нежелательно. К тому же KVM развивается компанией Red Hat в качестве основного средства виртуализации, поставляемого с Red Hat Enterprise Linux. Среда облачных вычислений в ИММ УрО РАН После ввода в эксплуатацию суперкомпьютера Уран вычислительный кластер предыдущего поколения стал малоинтересен пользователям, решающим вычислительные задачи, требующие высокой производительности. Таким образом, появилась возможность выделить из состава оборудования устаревшего кластера аппаратные средства, позволяющие развернуть среду для выполнения системы виртуальных машин. Для развертывания экспериментальной среды были выделены три узла кластера Fujitsu-Siemens Computers PRIMERGY RX330 S1 в следующей конфигурации двухъядерных процессора AMD Opteron 2218 с тактовой частотой 2,6 ГГц, имеется поддержка аппаратной виртуализации 8 ГБ оперативной памяти жесткий диск Seagate Barracuda ES 250GB Serial ATA II 7200RPM 16MB сетевая карта Broadcom BCM5715 Gigabit Ethernet. На каждый выделенный узел кластера установлена ОС Scientific Linux версии 6.1 . Конфигурация ОС и служб, необходимых для функционирования облачной инфраструктуры, типовая для всех узлов кластера. На жестком диске каждого узла сформирована LVM-группа логических томов vgnode следующего вида том vgnode-lvroot содержит корневой раздел, файловая система ext4, объем 50 ГБ том vgnode-lvhome содержит раздел home, файловая система ext4, объем 170 ГБ том vgnode-lvswap содержит раздел подкачки, объем 10 ГБ. Технология LVM позволяет динамически менять размер логических разделов и осуществлять горячее добавление новых томов в систему. Используемая файловая система ext4 является стандартом де-факто для современных дистрибутивов GNULinux. На каждом узле кластера подключены RPM-репозитории ELRepo и Fedora EPEL, установлено ПО QEMU, поддерживающее технологию виртуализации KVM, а также libvirt унифицированный набор инструментов для работы с виртуальными машинами. Подсистема безопасности SELinux была отключена из-за проблем, возникших при тестовых запусках виртуальных машин на узлах стандартные политики безопасности препятствуют функционированию OpenNebula в экспериментальном режиме. Служба управления облачной инфраструктурой OpenNebula и ее Web-интерфейс Sunstone в экспериментальном режиме запущены на виртуальной машине cf.imm.uran.ru в среде гипервизора VMware ESXi, работающего под управлением ОС CentOS 6 . ПО OpenNebula, включающее в себя службу oned и множество вспомогательных сценариев, было скомпилировано вручную и установлено в обособленную директорию varlibone. Служба oned и Web-интерфейс Sunstone запускаются при старте системы с реквизитами пользователя oneadmin. Доступ к Webинтерфейсу Sunstone организуется средствами балансировщика нагрузки haproxy. Хранилище образов виртуальных машин организовано на управляющей машине cf.imm.uran.ru. Двусторонний обмен образами виртуальных машин с узлами кластера осуществляется при помощи механизма SSH, реализованного в стандартной поставке пакета OpenNebula. Служба oned способна хранить служебные данные как при помощи встраиваемой СУБД SQLite, так и средствами MySQL клиент-серверной СУБД. В экспериментальном режиме данные хранятся в БД SQLite. В дальнейшем планируется создать выделенный сервер БД и провести миграцию данных на СУБД MySQL. Для полноценной эксплуатации облачной инфраструктуры OpenNebula необходимо обеспечить гипервизор виртуальных машин доступом к сетевому интерфейсу типа мост, что выполняется стандартными средствами Scientific Linux. Таким образом, каждая виртуальная машина обладает реальным IP-адресом и имеет доступ к вычислительной сети ИММ УрО РАН наравне с реальными серверами СКЦ. Инфраструктура OpenNebula не требует установки собственного ПО на рабочие узлы кластера весь обмен информацией в том числе и образов виртуальных машин выполняется исключительно при помощи протокола SSH, для передачи используется механизм SCP. Подразумевается, что управляющая машина способна без пароля подключиться к любому узлу кластера, используя имя пользователя oneadmin с домашней директорией varlibone. Возможность беспарольного неинтерактивного взаимодействия узлов и управляющей машины обеспечена при помощи RSA-ключей, сгенерированных при помощи OpenSSH, входящего в поставку Scientific Linux. В целях безопасности в службе sshd запрещена возможность аутентификации пользователя по паролю. Доступ к узлам кластера также выполняется при помощи собственных RSA-ключей пользователей внутри вычислительной сети ИММ УрО РАН. Пользователь oneadmin был включен в группу пользователей KVM. Декларирована локальная политика безопасности, позволяющая группе KVM получать доступ к средствам виртуализации libvirt. В настройках QEMU и libvirt запрещено изменение полномочий файлов образов виртуальных машин при запуске эмулятора. Мониторинг узлов кластера осуществляется периодическим опросом серверов со стороны управляющей машины. На каждый узел загружаются диагностические сценарии, написанные на языках Ruby и Shell. Результаты мониторинга возвращаются путем перенаправления потоков ввода-вывода. Аналогично выполняются все остальные операции на узлах с виртуальными машинами. Для функционирования OpenNebula необходим интерпретатор Ruby версии не ниже 1.8.7. Как на узлах кластера, так и на управляющей машине установлены Ruby 1.8.7p229 и средство управления gem-пакетами RubyGems 1.8.10. Система OpenNebula автоматически распределяет нагрузку между узлами кластера, прозрачную миграцию виртуальных машин, выполняет прочие административные функции. При корректной настройке инфраструктуры дальнейшее обслуживание со стороны системного администратора не требуется. В заключение отметим, что в настоящее время в СКЦ ИММ УрО РАН развернута экспериментальная версия облачной инфраструктуры на основе OpenNebula. В дальнейшем планируются работы по следующим направлениям сравнение производительности при использовании хранилища образов на основе SSHSCP с использованием сетевого хранилища на основе NASNFS миграция на клиент-серверную СУБД MySQL для повышения надежности облачной инфраструктуры разработка политики SELinux, специфичной для корректного и полноценного функционирования OpenNebula интеграция с Active Directory автоматизация процесса добавления нового узла в серверную ферму OpenNebula запуск PaaS-окружения Cloud Foundry на созданной облачной инфраструктуре апробация и предоставление Web-сервисов, масштабируемых при помощи OpenNebula. Кроме того, предусматриваются создание виртуального Windows-кластера, а также перенос в облачную среду ряда унаследованных слабонагруженных серверов. 