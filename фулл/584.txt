ОПТИМИЗАЦИЯ ПЕРЕУПОРЯДОЧИВАНИЯ МОДЕЛЬНЫХ ЧАСТИЦ   ПРИ РЕАЛИЗАЦИИ МЕТОДА ЧАСТИЦ В ЯЧЕЙКАХ НА GPU  

Представлено описание реализации метода частиц в ячейках на GPU. Основным недостатком метода, с точки  зрения затрат по времени, является функция переупорядочивания частиц между ячейками. Изложена оригинальная методика оптимизации данного этапа расчета, позволяющая избавиться от атомарных операций. Приведены результаты тестирования производительности на ряде современных графических процессоров.   

 Для численного моделирования плазмы часто используется метод частиц в ячейках англ. Particle-in-Cell method, PIC 1 2. Моделирование плазменной турбулентности может проводится либо в гидродинамическом приближении путем добавки в уравнения магнитной гидродинамики МГД дополнительных слагаемых, обеспечивающих аппроксимацию турбулентности, либо в кинетическом приближении путем решения непосредственно кинетического уравнения Власова или Больцмана. Первый вариант является в данном случае неприемлемым в силу того, что характер возникающей турбулентности экспериментально и теоретически слабо изучен, а также в силу того, что плазма в данном случае субтермо ядерная плазма установки ГОЛ-3, ИЯФ СО РАН 3 и плазма перспективных установок управляемого термоядерного синтеза не является даже приближенно равновесной, так что уравнения МГД не могут быть использованы. Кинетическое уравнение может быть решено либо методом частиц в ячейках, либо прямым конечно-разностным методом. В обоих случаях существует пространственная сетка для решения уравнений Максвелла. При использовании метода частиц в каждую ячейку сетки добавляются модельные частицы, уравнения движения которых представляют собой уравнения характеристик для кинетического уравнения Власова, при использовании прямого конечно-разностного метода вводится дополнительная сетка в пространстве скоростей, так что возникает сетка в 6-мерном пространстве. Таким образом, вариант с использованием метода частиц является более затратным по количеству операций, но значительно более экономичным по памяти по сравнению с прямым конечно-разностным методом. Итак, только метод частиц в ячейках обеспечивает возможность решения задачи. Все варианты построения более хорошего быстрого метода на данный момент связаны с внесением тех или иных некорректных упрощений в физическую постановку задачи. По сути дела, расчеты с использованием метода частиц в ячейках проводятся именно с целью проверки того, какая из упрощенных физических моделей МГД, модель на основе первых моментов уравнения Больцмана... будет в данном случае корректной. Вычислительные эксперименты показывают, что при реализации метода частиц в ячейках наиболее важной характеристикой оборудования является доступ к оперативной памяти. С этой точки зрения целесообразно использовать для реализации метода частиц в ячейках гибридные вычислительные системы на базе графических процессоров GPU 4, которые обладают высокой пропускной способностью по доступу к памяти за счет своей архитектуры и таких аппаратных средств, как текстурный кэш и разделяемая память. Кроме того, большое количество ядер на GPU, так же как и на ускорителях Intel Xeon Phi, может быть использовано для создания более быстрой реализации метода частиц, чем на обычных многоядерных процессорах, таких как Intel Xeon, Intel Nehalem, IBM Power и др. Кроме того, необходимость перехода на GPU диктуется тем, что среди наиболее мощных компьютеров мира имеется тенденция к увеличению доли гибридных суперЭВМ. В 2018 г. это выглядит следующим образом суперЭВМ на основе Intel Xeon Phi и Nvidia Volta или Nvidia Pascal в Top500 и Top10 1, 2, 5, 6, 7, 9 . Такая тенденция поддерживается тем, что энергоэффективность гибридных систем выше, чем систем, имеющих традиционную архитектуру. В последнее время появилось много программных пакетов для моделирования динамики плазмы на основе метода частиц в ячейках, использующих графические ускорители GPU, например, PIConGPU 5 и ALaDyn 6 и др. 7 8. Вместе с тем многие имеющиеся коды для расчета динамики плазмы на GPU ориентированы на конкретный вычислительный либо алгоритмический вопрос и с точки зрения физических расчетов носят предварительный характер. В итоге имеется или корректно работающий численный код, которому не хватает вычислительной мощности, или очень быстро работающий код для гибридной суперЭВМ, физическая корректность результатов которого вызывает сомнения, потому что в его разработке участвовали только специалисты по суперЭВМ. Описываемый в данной работе код разрабатывался при участии физиков 9 10, что освобождает его от упомянутых выше недостатков. Математическая модель высокотемпературной бесстолкновительной плазмы представляется кинетическим уравнением Власова и системой уравнений Максвелла 1, которые в безразмерной форме имеют следующий вид 1 0, 4 1, 1, 4, 0. Здесь индексами и помечены величины, относящиеся к ионам и электронам соответственно 1, функция распределения частиц, масса, импульс, положение иона или электрона, напряженности электрического и магнитного полей. Для перехода к безразмерному виду в качестве единиц используются следующие базовые величины скорость света 310 смс плотность плазмы 10 см плазменная электронная частота 5,610 c . В начальный момент времени в трехмерной области решения, имеющей форму прямоугольного параллелепипеда 0, 0, 0, находятся плазма, состоящая из электронов и ионов водорода, и пучок электронов. Заданы плотности электронов пучка и электронов плазмы 1 . Плотность ионов плазмы равна сумме плотностей электронов пучка и электронов плазмы. Температура электронов плазмы и пучка температура ионов считается нулевой 0. Начальное распределение частиц по скоростям максвелловское с плотностью распределения 1 exp, 2 где разброс частиц по скоростям, средняя скорость пучка. Средняя скорость ионов и электронов фона нулевая. Все частицы распределены по области равномерно, начальная средняя скорость пучка направлена по и равна 0,2. Граничные условия периодические. В расчетах представляет интерес развитие отдельно взятой неустойчивой моды, поэтому длина области в направлении выбрана равной одной длине исследуемой плазменной волны, 4 шаг сетки. Эти эксперименты описаны в статьях 9 10. Основываясь на книге 2, приведем описание идеи метода. Пусть задача записана в абстрактной операторной форме 0. 1 Здесь, вектор-функция со значениями в, вектор независимых переменных с областью изменения в, 0 . В реальных задачах роль уравнения 1 выполняет кинетическое уравнение уравнение Власова или Больцмана или уравнения гидродинамики. Решение задачи 1 представляется в виде следующей интерполяционной формулы, . Этот переход называют разбиением среды на модельные частицы. Функция, называется ядром или форм-фактором модельной частицы. Эта функция описывает распределение некоторого признака массы, заряда, скорости в рамках одной частицы. Далее, если представить функцию в виде, где радиус-вектор частицы, импульс частицы, то можно показать, что решение уравнения 1 тождественно решению следующей динамической системы,..., 1... 2 Здесь вектор обобщенного поля. Исключительно важно, что переход к модельным частицам не означает замены реальной физической системы, где число частиц молекул, атомов, ионов и пр. порядка числа Авогадро 6,0210 на некую упрощенную систему из значительно меньшего количества таких же частиц, но более крупного размера даже для самых больших расчетов на суперЭВМ на данный момент не превышает 10 . Система уравнений 2 является не более чем математическим формализмом для решения уравнения 1, т. е. на данном этапе никакого нарушения математической строгости не происходит, система уравнений 2 точно так же описывает физический процесс, как и исходное уравнение 1. Вначале выполняется задание начальной конфигурации распределения вещества в расчетной области и полей. Производится распределение частиц по ячейкам так, чтобы плотности или токи, вычисленные по частицам, соответствовали заданной конфигурации. Далее на каждом временном шаге процесса моделирования выполняется 1 вычисление токов по значениям координат и скоростей модельных частиц 2 расчет электрического и магнитного полей по токам 3 сдвиг частиц вычисление новых значений координат и скоростей модельных частиц с учетом новых значений поля. При разбиении пространства на ячейки на 3-м шаге возникает необходимость перераспределения частиц между ячейками. Для того чтобы подчеркнуть значение полученной на GPU производительности, вначале приведем данные о производительности на CPU для процессора Intel Xeon Е5540 на 4 его ядрах время вычисления сдвига частиц было в 41,6 раза больше, чем для GPU Nvidia Tesla K40, т. е. 11,7 с кинетический вариант развития неустойчивости, 1 000 модельных частиц в ячейке. Причина этого заключается в том, что частицы обычно, в большинстве программ на основе метода частиц, хранятся в памяти без учета их расположения в пространстве моделирования, т. е. следующая по номеру в массиве координат частица может быть расположена в пространстве моделирования очень далеко от предыдущей. Это значит, что значения полей при расчете сдвига частиц будут взяты из совершенно другой части трехмерного массива, в виде которого хранятся поля. Это, в свою очередь, означает, что использование кэш-памяти будет очень неэффективным. Поскольку еще в работе 11 было показано, что переход от хранения координат и скоростей частиц в одном массиве без учета расположения в пространстве 1 вариант к отдельным массивам по каждой ячейке 2 вариант приводит к ускорению движения частиц в 1,52 раза, что подтверждено и в данной работе см. таблицу, то частицы хранятся распределенными по ячейкам напомним, что количество частиц в ячейке около 1 000. Но при этом возникает проблема переупорядочивания модельных частиц, т. е. передачи частиц, перелетающих в другие ячейки, на хранение в массивы, связанные с этими другими ячейками см. рисунок, . Напомним, что по соображениям устойчивости метода частиц модельные частицы могут перелетать только в соседнюю ячейку. Конфликт чтения-записи см. рисунок, возникает в силу того, что каждое перемещение частицы выполняется отдельным потоком CUDA, при этом необходимо считать частицу из массива частиц соседней ячейки и удалить ее оттуда, т. е. выполняется и чтение, и запись одновременно несколькими потоками, а для мощных GPU Volta, Pascal и др. несколькими тысячами потоков. Для разрешения конфликта в данном случае используются атомарные операции CUDA, что существенно замедляет выполнение алгоритма в целом. Поэтому есть необходимость выполнять операцию переупорядочивания по-другому. В данной статье предложен альтернативный более быстрый вариант. Время выполнения различных этапов вычислительного алгоритма на нескольких GPU оптимизированный вариант без атомарных операций по сравнению с неоптимизированным полужирный шрифт вариантом на Kepler K40. Время указано в микросекундах Execution time for different stages of the computational algorithm for various GPUs optimized version without atomic operations compared to non-optimized algorithm on Kepler K40 bold font. Time is given in microseconds Этап алгоритма GPU Kepler K40 Kepler K80 Pascal P100 Сдвиг частиц 433.8 348.06 338.1 Расчет поля 31.7 25.4 19.3 Переупорядочивание 1131 446.43 98.9 1 2 3 Следующая схема предложена для сокращения времени переупорядочивания модельных частиц 1 функция, исполняемая на GPU и производящая переупорядочивание, разделяется на две части 2 в каждой ячейке формируется список модельных частиц, которые должны быть перемещены в каждую из соседних ячеек 3 заполняется матрица размером 3, в которой записано, сколько модельных частиц должно быть перемещено в каждую из соседних ячеек всего таких ячеек 26 4 перемещаемые частицы копируются в буфера, соответствующие соседним ячейкам см. рисунок, . При этом ни разу не возникает конкуренция за доступ к памяти, именно поэтому удается избавиться от использования атомарных операций. 