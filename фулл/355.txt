АЛГОРИТМ И ПРОГРАММНАЯ РЕАЛИЗАЦИЯ ГИБРИДНОГО МЕТОДА ОБУЧЕНИЯ ИСКУССТВЕННЫХ НЕЙРОННЫХ СЕТЕЙ 

Задача обучения искусственной нейронной сети может рассматриваться как задача оптимизации, при этом основная проблема заключается в выборе из разнообразных оптимизационных методов наиболее подходящего. Выбор в пользу градиентных методов обоснован тем, что, как правило, в задачах обучения нейросетей целевую функцию можно выразить в виде дифференцируемой функции от всех весовых коэффициентов. Однако сложный характер этой зависимости приводит к тому, что целевая функция имеет локальные экстремумы и седловые точки, а потому делает применение градиентных методов не всегда обоснованным. Для решения задач оптимизации с многоэкстремальным критерием используют методы случайного поиска, к которым относятся генетические алгоритмы, обычно отличающиеся медленной сходимостью. Для проведения сравнительной характеристики градиентных методов и генетического алгоритма разработано ПО с веб-интерфейсом. В качестве задачи для обучения нейронной сети использовалась задача аппроксимации двумерной функции Розенброка. Результаты исследования показали, что градиентные методы имели быструю сходимость лишь в начале обучения, а генетический алгоритм – в конце. Таким образом, предложен гибридный алгоритм, основанный на последовательном использовании градиентных методов и генетического алгоритма.

 Обучение искусственной нейронной сети ИНС может рассматриваться как задача оптимизации, при этом основная проблема заключается в выборе наиболее подходящего оптимизационного метода 1. Выбор в пользу градиентных методов обоснован тем, что, как правило, в задачах обучения ИНС целевую функцию можно выразить в виде дифференцируемой функции от всех весовых коэффициентов. Однако сложный характер зависимости от весовых коэффициентов приводит к тому, что целевая функция имеет локальные экстремумы и седловые точки, что делает применение градиентных методов не всегда обоснованным. Для решения задач оптимизации с многоэкстремальным критерием используют методы случайного поиска, к которым относятся генетические алгоритмы. Однако генетические алгоритмы обычно отличаются медленной сходимостью. Большего успеха можно достигнуть, используя в одном алгоритме обучения и градиентные, и генетические методы. Неопределенность выбора метода обучения обусловлена широким классом градиентных и генетических алгоритмов. В автоматизированных системах нейро-сетевого программирования следует стремиться к сокращению неопределенности. В статье рассматривается параметризация алгоритмов обучения с целью сокращения неопределенности выбора и предлагается соответствующее ПО. Общий анализ градиентных методов обучения нейронных сетей позволяет утверждать, что любой из этих методов можно представить как частный случай адаптивного алгоритма 2. Параметрами алгоритма являются порядок m и последовательности,. Рассмотрим обучение многослойного персептрона методом обратного распространения с адаптивным алгоритмом минимизации функции ошибки. 1. Находим начальные значения параметров стартовую точку w, начальное направление движения p и шаг.2. Выбираем очередной вектор из обучающего множества и подаем его на вход сети. 3. Определяем направление движения p по формуле . 4. Вычисляем критерий остановки, например среднюю квадратичную ошибку 1, 3. 5. Если условие остановки выполняется, переходим к шагу 6, если нет переходим к шагу 2. 6. Конец алгоритма. В результате получаем обученную сеть. Основные недостатки данного алгоритма обучения паралич сети, попадание в локальные минимумы, многократное предъявление всего обучающего множества 4. Генетический алгоритм является итерационным и часто вычисляет решение в некоторой окрестности глобального минимума. В связи с этим он может применяться в задачах подстройки весов при обучении ИНС. К преимуществам генетического алгоритма относится параллельная обработка множества возможных решений для нахождения глобального экстремума многоэкстремальной функции. При этом поиск концентрируется на наиболее перспективных из них. Мутация при обучении ИНС необходима, так как она позволяет выходить популяции из локального экстремума, а также является способом получения новых, более здоровых особей. На рисунке 1 показана ИНС типа многослойный персептрон. Хромосома составляется из весовых коэффициентов слева направо и сверху вниз. Так как генетический алгоритм не является строго детерминированным, скрещивание и мутацию в пределах одной эпохи можно проводить в произвольном порядке. Для каждого гена выбранной особи с вероятностью P произведем мутацию. Таким образом, к параметрам генетического алгоритма относятся размер популяции, число потомков, число хромосом для мутаций, вероятности выбора хромосомы и вероятность мутации гена. Соответствующий подбор параметров позволяет выделить генетический алгоритм из широкого класса алгоритмов. Для гибридного обучения ИНС получаем две альтернативы. В первом случае начнем обучение адаптивным алгоритмом и, достигнув критерия перехода, продолжим обучение генетическим алгоритмом, добавив к первой популяции здоровую особь ИНС, обученную адаптивным алгоритмом. Критериями перехода от адаптивного к генетическому методу обучения для гибридного алгоритма являются количество эпох обучения A и среднеквадратическая ошибка E. Второй альтернативной реализацией гибридного алгоритма будет обучение ИНС генетическим алгоритмом. В результате работы генетического алгоритма получим популяцию, наилучший представитель которой является одним из возможных решений задачи обучения. Если значение критерия обучения удовлетворяет заданному качеству, можно считать, что процесс обучения завершен. Если это не так, продолжим обучение адаптивным алгоритмом. Критериями перехода от генетического к адаптивному методу для второго варианта гибридного алгоритма являются количество эпох обучения A, среднеквадратическая ошибка E, стационарность популяции. Рассмотрим алгоритм предлагаемого гибридного метода обучения варианта адаптивный генетический. 1. Создаем ИНС с первоначальной инициализацией весовых коэффициентов. 2. Обучаем ИНС описанным выше адаптивным алгоритмом, пока не будет достигнут критерий перехода к генетическому методу обучения. 3. Создаем популяцию из N1 особей. В первую популяцию добавим ИНС, обученную адаптивным алгоритмом. 4. Произведем скрещивание особей с вероятностью выбора пары P. От каждой пары получим S потомков. Для определения генов потомка воспользуемся формулой . 5. Выберем из новой популяции лучших N особей. 6. Если лучший представитель особи соответствует заданному качеству обучения, переходим к шагу 9. 7. Проведем мутацию для особей, выбранных с вероятностью P. Для каждого гена выбранной особи с вероятностью P произведем мутацию. Для мутации гена воспользуемся формулой . 8. Если лучший представитель особи соответствует заданному качеству обучения, переходим к шагу 9, если нет возвращаемся к шагу 4. 9. Конец алгоритма. В результате получена обученная сеть . Окончательный выбор алгоритма обучения зависит от конкретной задачи. Поэтому для тестирования алгоритмов был реализован эмулятор ИНС в виде веб-приложения. Языком программирования выбран PHP. На рисунке 2 приведена диаграмма классов методов обучения реализованного программного продукта. Родителем для всех классов-методов обучения является abstract class Method. Основные потомки данного класса метод обратного распространения ошибки class MethodBP генетический метод обучения class MethodGA гибридный метод обучения class MethodHybridBPGA. Гибридный метод инкапсулирует в себе два объекта методов class MethodBP и class MethodGA. Алгоритмы оптимизации функции ошибки представлены абстрактным классом Optimizer, классами OptimizerGradient и OptimizerGradientAdaptive . Функция ошибки нейрона представлена классом FuncOptimizerMNK, который реализует функцию наименьших квадратов. На рисунке 3 изображена диаграмма основных классов архитектуры ИНС. Архитектура многослойного персептрона реализована классами Net, Layer, Neuron и дополнительным классом нейрона NeuronBP, который применяется в методе обратного распространения ошибки. В отличие от класса Neuron он заключает в себе функцию ошибки и алгоритм оптимизации функции ошибки. Классы Activate и ActivateSigmoid реализуют активационную функцию нейрона, в данном случае сигмовидную. Были проведены эксперименты с обучением ИНС топологии многослойный персептрон. В качестве задачи для обучения выбрана задача аппроксимации двумерной функции Розенброка 3, x, y0 3. Обучающая выборка состояла из 155 примеров, размер входного вектора 2, выходного 1. Критериями остановки обучения были порог в 1000 эпох и среднеквадратическая ошибка, составляющая 0,001. Для экспериментов создано десять ИНС. В анализе участвовали следующие алгоритмы простой градиентный спуск, адаптивный алгоритм, генетический алгоритм, адаптивныйгенетический, генетическийадаптивный. Результаты исследования представлены в таблицах 1 и 2. Проведенные эксперименты показали, что с задачей справились генетический и гибридный алгоритмы версии адаптивныйгенетический. Для данной задачи обучения быстрее всего сходились градиентные методы, но до определенной среднеквадратической ошибки, в среднем до 0,003. Порог в 0,001 за 1 000 эпох не преодолел ни один градиентный метод. Из этого следует, что целесообразнее всего в начале обучения использовать адаптивный метод, который достаточно быстро находит решение со среднеквадратической ошибкой 0,003, а далее применять генетический алгоритм. По количеству эпох генетический алгоритм в некоторых случаях быстрее гибридного. Но для данной задачи время одной эпохи генетического алгоритма гораздо выше среднего времени одной эпохи гибридного . Поэтому можно сделать вывод, что лучше всего для данной задачи подходит гибридный алгоритм варианта адаптивныйгенетический. Таким образом, получена параметрическая модель обучения ИНС, содержащая широкий спектр известных алгоритмов обучения и позволяющая настроить параметры для наилучшего решения поставленной задачи. 