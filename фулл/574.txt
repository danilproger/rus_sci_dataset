ПОДХОД К ПОСТРОЕНИЮ РАСШИРЕННЫХ   ТЕМАТИЧЕСКИХ МОДЕЛЕЙ ТЕКСТОВ НА РУССКОМ ЯЗЫКЕ 

Представлен новый подход для получения расширенных тематических моделей текстов научных статей   на русском языке. Под расширенной моделью понимается тематическая модель, содержащая кроме однословных  терминов термины, состоящие из нескольких слов (также называемые многословные термины или ключевые фразы). Такие модели лучше интерпретируются пользователями и точнее описывают предметную область документа,  чем модели, состоящие только из униграмм (отдельных слов).  На основе предложенного подхода была разработана система, в результате работы которой для каждого документа предоставляется набор содержащихся в нем тем с указанными вероятностями, ключевыми словами и фразами для каждой темы.  Предложенный в статье подход может быть полезен при построении рекомендательных систем и систем автореферирования.  : тематические модели, обработка текста, извлечение ключевых слов, извлечение многословных терминов, определение темы текста.  

В современном мире непрерывно производятся огромные объемы электронной информации. Значительную ее часть составляют тексты на естественном языке. В связи с этим становится все более актуальной задача автоматической обработки таких текстов с целью извлечения из них структурированных данных, пригодных для дальнейшего использования в машинном анализе. Одним из современных инструментов обработки естественного языка являются тематические модели. Тематическое моделирование заключается в построении модели некоторой коллекции текстовых документов. В такой модели каждая тема представляется дискретным распределением вероятностей слов, а документы дискретным распределением вероятностей тем 1. Следует обратить внимание на то, что нельзя смешивать понятия тематического моделирования и тематической классификации. Основное отличие состоит в том, что при определении тем текстов отсутствует какая-либо информация о темах неизвестно ни их количество, ни их содержание что подразумевается под каждой темой. Для классификации же необходимы априорные знания о структуре классов. В этом смысле процесс тематического моделирования больше похож на кластеризацию, чем на классификацию. Однако ни классификация, ни кластеризация не справляются с синонимией и полисемией, в отличие от тематического моделирования. А, как известно, важнейшим препятствием при создании систем автоматической обработки текстов является лексическая неоднозначность. Так, в тематической модели слова, являющиеся синонимами, с большой вероятностью попадут в одну и ту же тему, так как зачастую они используются в одинаковом контексте. В то же время омонимы слова одинаковые по написанию, но имеющие разное значение с большой вероятностью будут отнесены к разным темам, так как обычно контексты их использования не совпадают. В данной статье описан новый подход для получения расширенных тематических моделей текстов научных статей на русском языке. Под расширенной моделью здесь понимается тематическая модель, содержащая помимо однословных терминов термины, состоящие из нескольких слов также называемые многословными терминами или ключевыми фразами. Такие модели лучше интерпретируются пользователем и точнее описывают предметную область документа, чем модели, состоящие только из униграмм отдельных слов. Тематическое моделирование построение тематической модели некоторой коллекции текстовых документов. Тематическая модель представляет собой описание коллекции с помощью тематик, использующихся в документах этой коллекции, и определяет слова, относящиеся к каждой из тематик 1. Вероятностная тематическая модель представляет каждую тему как дискретное распределение на множестве слов, а документ как дискретное распределение на множестве тем 2. Одной из разновидностей тематических моделей являются тематические модели, выявляющие ключевые фразы термины предметной области. Под ключевой фразой в данной работе подразумевается устойчивая последовательность слов -грамма, имеющая определенную семантику в контексте заданной предметной области, относящаяся к одной из выявленных в тексте тем и обладающая значительной частотой встречаемости по сравнению с другими -граммами. Пусть задана некоторая коллекция документов, тогда множество всех встречающихся в данной коллекции терминов слов или -грамм. Каждый документ представляется в виде последовательности терминов,..., длиной, при этом каждый термин может встретиться в документе несколько раз. Предполагается, что существует некоторое множество тем, причем каждое вхождение термина связано с некоторой темой . Коллекция документов рассматривается как множество троек, выбранных случайно и независимо из дискретного распределения, заданного на конечном множестве . При этом документы и термины являются наблюдаемыми переменными, а тема скрытой переменной. Гипотеза о том, что элементы выборки независимы, эквивалентна предположению мешка слов порядок слов в тексте документа не имеет значения, и тематику можно выявить даже при произвольной перестановке терминов в тексте. В этом случае каждый документ можно представить как подмножество, в котором в соответствие с каждым элементом поставлено количество вхождений термина в документ . Согласно определению условной вероятности, формуле полной вероятности и гипотезе условной независимости . Тогда задача построения тематической коллекции документов заключается в нахождении для известной коллекции множества всех использующихся в ней тем, а также для каждого по распределению слов по документам восстановить распределения тем в документе и слов по темам . В настоящее время тематические модели находят применение в самых различных областях. К примеру, в 3 авторы используют тематическое моделирование с помощью алгоритма Latent Dirichlet Allocation LDA на отзывах пользователей для создания персонализированных медицинских рекомендаций. В работе 4 авторы используют тематическую модель, включающую в себя авторов, тексты и цитирования, для библиографического анализа. Также тематическое моделирование применяется в обучении в работе 5 авторы предлагают использовать тематическое моделирование для упрощения оценки учителем письменных работ учеников. Помимо этого, тематическое моделирование применяется для анализа данных социальных сетей 68, для многоязычного информационного поиска 9, выявления трендов в новостных потоках или научных публикациях 10, для автоматического присвоения тегов веб-страницам 11, в рекомендательных системах, учитывающих контекст 12, в анализе террористической активности в сети Интернет 13 и мн. др. Современные требования к тематическим моделям довольно разнообразны. Основное из них заключается в том, что тематические модели должны хорошо поддаваться интерпретации, конечному пользователю должны быть понятны причины выделения определенных тем в тексте и структура самих тем. Эта особенность является главным преимуществом тематических моделей перед набирающими популярность нейронными сетями. Кроме того, часто требуется, чтобы тематические модели учитывали разнородные данные, выявляли динамику тем во времени, автоматически разделяли темы на подтемы, использовали не только отдельные ключевые слова, но и многословные термины и т. д. Основными подходами к тематическому моделированию являются алгоритмы PLSA Probabilistic Latent Semantic Analysis, вероятностный латентный семантический анализ, LDA Latent Dirichlet Allocation, латентное размещение Дирихле и библиотека ARTM Additive Regularization for Topic Modeling, аддитивная регуляризация тематических моделей. PLSA вероятностная тематическая модель представления текста на естественном языке. Модель называется латентной, так как предполагает введение скрытого латентного параметра, являющегося темой. Впервые описана Томасом Хофманном в 1999 г. 14. LDA модель, позволяющая объяснять результаты наблюдений с помощью неявных групп, благодаря чему возможно выявление причин сходства некоторых частей данных. Например, если наблюдениями являются слова, собранные в документы, утверждается, что каждый документ представляет собой смесь небольшого количества тем и появление каждого слова связано с одной из тем документа 15. ARTM является обобщением большого числа алгоритмов тематического моделирования, позволяет комбинировать регуляризаторы, тем самым комбинируя тематические модели. При таком подходе PLSA представляет собой тематическую модель без регуляризаторов, а LDA тематическую модель, в которой каждая тема сглажена одним и тем же регуляризатором Дирихле. Модель ARTM в предложена 2014 г. 16. В настоящее время ARTM приобретает все большую популярность благодаря своей универсальности и гибкости настройки параметров моделей. Как уже говорилось, основным требованием к тематическим моделям является их интерпретируемость. При этом в большинстве алгоритмов тематического моделирования в качестве терминов используются только слова, а не -граммы. В то же время для человека использование ключевых фраз для обозначения тем может упростить интерпретацию выявленной темы и разрешить возможную неоднозначность. При этом стоит заметить, что в русском языке задача извлечения ключевых фраз является гораздо более сложной, чем, например, в английском. Это связано с тем, что русский язык флективный, т. е. каждое слово в речи может быть представлено множеством различных словоформ. Обычные алгоритмы извлечения ключевых фраз, основанные на относительной частоте встречаемости -грамм в документах, показывают низкий уровень точности извлечения. Каждую словоформу такие алгоритмы воспринимают как различные термины, и из-за этого частота встречаемости снижается в несколько раз. Существует несколько основных подходов к решению данной проблемы. Во-первых, для распознавания словоформ можно использовать словари, содержащие все возможные формы слова 17. Очевидно, что в этом случае точность определения будет высокой для имеющихся в словаре слов. Однако очевидно, что применимость словарных алгоритмов ограничена предметной областью словаря. Другой подход к этой задаче использование лексико-синтаксических шаблонов 18 19. В 18 описана стратегия распознавания в заданном тексте фрагментов, соответствующих заданному лексико-синтаксическому шаблону, предложен язык записи шаблонов, позволяющий задавать лексические и грамматические свойства входящих в него элементов. В статье 19 приводится описание системы с возможностью ручной настройки видов шаблонов для извлечения словосочетаний с помощью набора морфологических признаков. К сожалению, основными недостатками методов, основанных на шаблонах, является их большая трудоемкость. Проблему многословных терминов можно обойти, если использовать стемминг нахождение основы слова или лемматизацию приведение слова к его начальной форме. Однако тогда возникает проблема с восстановлением изначальных словосочетаний так, биграмма будет после стемминга выглядеть как тематическ моделировании, а после лемматизации как тематический моделирование. Очевидно, такие биграммы не могут быть использованы в качестве ключевых фраз в научной статье или на веб-странице, и для дальнейшего использования нужно преобразовать их в изначальное словосочетание. Для решения проблемы согласования словосочетаний применялись лексико-синтаксиче ские шаблоны. Исследование многословных ключевых терминов, выбранных для статей авторами, позволило составить базовый набор шаблонов. Мы не можем утверждать, что этот набор является полным, так как для составления полного набора шаблонов понадобилось бы привлечь экспертов-лингвистов с целью проведения дополнительного исследования. По этой причине вопрос о полноте набора шаблонов терминов пока остается открытым. Однако предусмотрено возможное расширение набора шаблонов, и в случае увеличения их количества потребуются лишь минимальные изменения в модуле согласования словосочетаний. Выделенные шаблоны удобно записать при помощи логики предикатов первого порядка. Рассмотрим словарь множество слов коллекции документов. Пусть,...,..., множество прилагательных из,...,..., множество существительных из . Для морфологических признаков введем следующие обозначения, содержит информацию о категории рода мужской, женский, средний, о категории числа единственное, множественное, о категории падежа именительный, родительный, дательный, винительный, творительный, предложный. Далее введем четырехместные предикаты, для прилагательных и, для существительных. Теперь шаблоны многословных терминов можно записать в виде формул исчисления предикатов, т. е. в случае согласованных словосочетаний будут истинны следующие шаблоны. 1., . Например, линейное уравнение. 2., . Например, разработка системы. 3., . Например, гипотеза условной независимости. 4., . Например, вероятностная тематическая модель. 5., . Например, определение тематики документа. 6., . Например, общая теория относительности. 7., . Например, умножение столбиком. 8., . Например, решение методом прогонки. Обобщение шаблонов 1 и 4 можно переписать в виде, . Обобщение шаблонов 2 и 5 запишем в виде, . Был разработан модуль согласования словосочетаний на основе вышеперечисленных шаблонов, использующий для извлечения морфологической информации программу Mystem . На вход модулю подаются лемматизированные словосочетания, которые сопоставляются с каждым шаблоном из набора. После определения требуемого шаблона словосочетание приводится в согласованный вид путем преобразования зависимых слов в форму, обусловленную формой главного слова и видом связи в словосочетании. Данный модуль показывает приемлемые результаты, а набор модулей покрывает значительную часть используемых в качестве ключевых фраз многословных терминов. Для улучшения результатов работы можно использовать как расширение набора шаблонов, так и дополнительные способы согласования. Основным недостатком текущей версии модуля является невозможность построения словосочетаний, в которых существительные находятся во множественном числе. Для решения данной проблемы в дальнейшем планируется использовать модуль поиска начальной формы из базового подхода, модифицировав его для поиска всех вариантов заданного лемматизированного словосочетания, а затем применить морфологический анализатор для определения нужного числа существительного. Также к недостаткам модуля можно отнести несовершенство изменения формы слов с точки зрения лингвистики. В русском языке множество исключений, например, слова, оканчивающиеся на -мя время, пламя и др. не относятся к первому, второму или третьему склонению, а склоняются смешанным способом, причем при склонении к корню добавляется -ен времени, пламени. Этот вид исключений был учтен в разработанной программе, однако, чтобы учесть все варианты исключений, встречающихся в русском языке, потребуется участие эксперта-лингвиста. Для лемматизации текста и построения морфологического словаря коллекции документов используется программа Mystem. Программа лемматизирует слова, используя анализ контекста для снятия лексической неоднозначности, а также предоставляет морфологическую информацию часть речи, род, число, падеж, склонение и др. для каждого слова. Программа распространяется бесплатно для некоммерческого использования. Выбор методов тематического моделирования объясняется наличием определенных особенностей. Для сравнения некоторые из них приведены в табл. 1. Сравнение методов тематического моделирования Название метода Увеличение количества параметров модели с ростом числа документов Применимость к большим наборам данных Использование многословных терминов Единственность и устойчивость решения PLSA да, есть линейная зависимость нет нет нет LDA нет да нет нет ARTM нет да нет да Также для выбора базового алгоритма построения униграммных тематических моделей был проведен ряд экспериментов. Была подготовлена коллекция текстов научных статей на русском языке на основе выложенных в открытом доступе архивов журналов Программные продукты и системы, Сибирский психологический журнал и Cloud of Science . Статьи очищены от формул, таблиц, рисунков и библиографических ссылок, аннотация и ключевые слова были удалены. Размер коллекции составляет более двухсот шестидесяти текстов. Для оценки результатов были выбраны следующие метрики, реализованные в библиотеке BigARTM и описанные в работе 20 перплексия, разреженность матриц и, доля фоновых слов, мощность ядер тем, чистота ядер тем, контрастность ядер тем. Первоначальные эксперименты выявили, что LDA показывает значительно худшие результаты перплексии по сравнению с PLSA и ARTM. В связи с этим дальнейшее сравнение проводилось только для двух последних алгоритмов при числе проходов по коллекции 100. Результаты представлены в табл. 2. Сравнение алгоритмов PLSA и ARTM Метрика PLSA ARTM Перплексия 754.784 751.888 Разреженность матрицы 0.769 0.769 Разреженность матрицы 0.000 0.635 Доля фоновых слов 0.059 0.050 Средняя чистота ядер тем 0.370 0.364 Средняя контрастность ядер тем 0.787 0.788 Средняя мощность ядер тем 2085.000 2085.600 По результатам эксперимента, приведенным в табл. 2, можно увидеть, что ARTM показывает аналогичные либо лучшие результаты по сравнению с PLSA для всех метрик, за исключением средней чистоты ядер, где ухудшение незначительно. В совокупности с особенностями алгоритмов, приведенными в табл. 1, было принято решение использовать в качестве алгоритма построения униграммных тематических моделей алгоритм ARTM в реализации библиотеки BigARTM 16. Для извлечения многословных терминов из текстов используется адаптированный алгоритм извлечения ключевых слов Turbotopics. Суть оригинального алгоритма Turbotopics, описанного в работе 21, обобщенно состоит в следующем. Первоначально строится униграммная модель текста при помощи алгоритма LDA. Затем производится расширение модели многословными терминами. Для каждого отдельного ключевого слова, полученного при помощи LDA, или уже добавленной фразы осуществляется проверка в исходном тексте на наличие соседних слов, которые с высокой вероятностью будут предшествовать в тексте или следовать за ним. Пара таких найденных слов, или, считается многословным термином и добавляется к списку ключевых фраз. Данный алгоритм был разработан для применения в текстах на английском языке на основе алгоритма построения тематических моделей LDA и показал довольно хорошие результаты. Поэтому в данной работе он был адаптирован для работы с русскими текстами с использованием алгоритма ARTM библиотеки BigARTM. Для определения списка ключевых слов для каждого документа изначально предполагалось использовать список наиболее часто встречающихся терминов однои многословных для каждой темы, к которой относится данный документ. Однако этот подход привел к тому, что из документа извлекались ключевые слова темы, а не самой статьи для различных документов списки ключевых слов были очень похожи, а термины, которые должны быть ключевыми исходя из текста статьи, не попадали в список из-за низкой частоты встречаемости. Для решения данной проблемы было предложено использовать TF-IDF статистическую меру, оценивающую важность каждого слова для документа, в котором оно встречается 22. Наибольшее значение TF-IDF будут иметь слова, которые часто встречаются в данном документе, но редко встречаются в остальных документах коллекции. В рамках исследования была разработана система, позволяющая строить расширенные тематические модели, включающие многословные термины, для коллекций научных статей на русском языке. Система написана на языке Python 3 с использованием библиотеки BigARTM. Используемые в системе алгоритмы из этой библиотеки были настроены таким образом, чтобы получить оптимальные результаты относительно различных метрик перплексия, разреженность и др. при использовании текстов научных статей на русском языке. Обобщенная схема работы системы представлена ниже. Далее приведено подробное описание процесса построения расширенной тематической модели и извлечения ключевых фраз разработанной системой. Опишем схему работы системы как последовательность шагов. На вход системе подается коллекция документов в формате .txt. Каждый документ должен быть представлен одним файлом, все документы помещены в одну директорию, путь к которой передается программе в качестве параметра. В модуле предобработки текста каждый документ очищается от специальных символов отличных от кириллических и латинских букв, из документа удаляются стоп-слова, все слова приводятся к нижнему регистру. Далее строится корпус коллекции в формате последовательного Vowpal Wabbit. Производится вызов программы Mystem, на вход которой подается файл с построенным на предыдущем этапе работы корпусом. Результатом работы является файл лемматизированного корпуса формат, аналогичный полученному ранее корпусу, только каждое слово заменено его начальной формой, а также файл морфологического словаря, где каждой строке соответствует слово и описывающая его морфологическая информация. На лемматизированном корпусе производится поиск ключевых слов и -грамм с помощью алгоритма Turbotopics. Найденные алгоритмом Turbotopics -граммы преобразуются из лемматизированного вида в согласованный с использованием шаблонов, описанных выше, и морфологического словаря, полученного на шаге 2. Для лемматизированного корпуса строится тематическая модель коллекции документов с использованием алгоритма ARTM. Параметры алгоритма можно подобрать автоматически или использовать заранее вычисленные так как подбор параметров задача весьма трудоемкая и занимает значительное время. Полученная на шаге 5 тематическая модель расширяется с помощью многословных терминов, извлеченных из коллекции на шаге 3 и согласованных на шаге 4. Для каждого документа строится словарь TF-IDF с каждым словом в лемматизированном документе сопоставляется значение меры TF-IDF. Слова в словаре сортируются по убыванию значения меры. На основе матрицы распределения тем по документам с каждым документом сопоставляется набор присутствующих в нем тем и их вероятностей учитываются только темы, вероятность появления которых в данном документе превышает порог 1, где количество тем в модели. После этого сравниваются два множества первые слов из отсортированного словаря TF-IDF и первые слов и словосочетаний для каждой темы, отсортированных по вероятности встретить этот термин в документе. Итоговыми ключевыми словами для темы документа будет пересечение этих множеств. и могут настраиваться по умолчанию эти значения равны 100 и 300 соответственно. Такие значения параметров были подобраны эмпирическим путем, чтобы каждому документу в среднем соответствовало порядка 510 ключевых слов и фраз. Результатом работы программы является текстовый файл, содержащий следующую информацию название исходного документа список тем, для каждой из которых указана вероятность содержания ее в тексте как десятичная дробь от 0 до 1 список ключевых слов и фраз для каждой темы. Также для пользователя доступен файл с описанием тем, где с каждой темой сопоставлено множество слов и словосочетаний с наибольшей вероятностью для этой темы. Поскольку невозможно автоматически оценить интерпретируемость тем и приемлемость извлеченных ключевых фраз, результаты были оценены вручную. Далее приведены несколько примеров работы алгоритма для различных публикаций разной направленности. Некоторые из наиболее частотных слов и фраз для первых пяти тем расширенной тематической модели коллекции, представлены в табл. 3. Расширенная тематическая модель коллекции научных статей Тема Расширенная тематическая модель Тема 1 алгоритм, решение, задача, значение, вершина, значение параметра, время распознавания, класс объекта, обработка информации, алгоритм поиска, вершина графа, изображение объекта, граница решения, задача поиска, граф решения Тема 2 метод, данные, алгоритм, классификация, текст, слово, классификатор, обучение, значение параметра, класс объекта, множество признака, представление текста, процесс обучения, метод классификации, построение модели, задача классификации, качество классификации, обучение классификатора, классификация текста Тема 3 человек, ребенок, психологический, группа, отношение, стратегия восприятия, процесс формирования, образ мира, группа испытуемая, уровень развития, респондент группы, развитие ребенка Тема 4 система, управление, процесс, модель, требование, разработка, система управления, орган управления, процесс разработки, модель прогнозирования, критерий эффективности проекта, этап прогнозирования, критерий эффективности, эффективность проекта Тема 5 исследование, отношение, испытуемый, элемент, диагностический, результат исследования, значение параметра, удовлетворенность отношения, процесс формирования, поиск решения, вид деятельности, группа испытуемая, удовлетворенность брака, формирование религиозности По представленным в табл. 3 результатам можно отметить, что темы из разных предметных областей технические науки и психология очень хорошо различимы в тематической модели. При этом граница между более узкими темами не настолько четкая если тема 4 довольно хорошо интерпретируется как отдельная предметная область, связанная с управлением проектами и процессом разработки, темы 1 и 2 связаны с классификацией и распознаванием, а темы 3 и 5 с психологической диагностикой. При этом важно заметить, что в теме 5 многословные термины удовлетворенность отношения, формирование религиозности и т. д. улучшают интерпретируемость темы как относящуюся к психологии, тогда как термины исследование, испытуемый являются более общими. В табл. 4 представлены извлеченные программой ключевые слова и фразы для нескольких научных публикаций. Ключевые слова и фразы Название статьи Извлеченные ключевые слова и фразы 1 Алгоритм детектирования объектов на фотоснимках с низким качеством изображения объект, класс, изображение, набор, автокодировщик, обучение, объект, класс, набор, изображение, слой, пиксел 2 Проектирование интерфейса программного обеспечения с использованием элементов искусственного интеллекта программный, пользователь, система управления, уровень развития, нечеткий, интерфейс, характеристика, эксперт, система управления 3 Родительское отношение как фактор формирования религиозности личности ребенок, отношение, родитель, формирование, религиозность, религиозный, религия, семья, родительский, решение задачи 4 Прогнозирование платежеспособности клиентов банка на основе методов машинного обучения и марковских цепей прогнозирование, состояние, клиент, классификатор, ак, заемщик, решение задачи, дерево решения 5 Разработка системы хранения ансамблей нейросетевых моделей данные, модель, набор, ансамбль, ряд, преобразование, хранение, нейросетевой, оценка качества, процесс формирования, классификация текста Можно утверждать, что извлеченные ключевые слова и фразы соответствуют содержанию статей и хорошо определяют предметную область исследований. При этом можно заметить, что в некоторых случаях они дают большее представление о содержании публикации, чем ее название например, ключевая фраза дерево решения дает понять, что в качестве алгоритма машинного обучения в четвертой статье использовались деревья решений, а ключевая фраза классификация текста в статье 5 указывает, что ансамбли нейросетевых моделей здесь использовались для классификации текста а не только изображений, например. Тематические модели позволяют автоматически систематизировать большие коллекции текстовых документов на естественном языке, повышают эффективность информационного поиска. В ходе данного исследования была разработана система построения тематических моделей и извлечения ключевых слов и фраз для текстов научных статей на русском языке. Для проведения экспериментов была подготовлена коллекция очищенных текстов научных статей на русском языке из размещенных в открытом доступе журналов . Разработанная система способна строить расширенные тематические модели, включающие, помимо униграмм, словосочетания в согласованном виде. Для каждого документа предоставляется набор содержащихся в нем тем с указанными вероятностями и ключевыми словами и фразами для каждой темы. Благодаря расширению тематической модели многословными терминами темы хорошо интерпретируются. Извлекаемые ключевые слова и фразы соответствуют содержанию документа. Предложенный в статье подход может быть полезен при построении рекомендательных систем и систем автореферирования. 